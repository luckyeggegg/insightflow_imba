name: Deploy S3 buckets & Module-data_ingestion (batch & streaming)

on:
  workflow_dispatch:
  push:
    branches: [main, feat_data_ingestion_batch, feat_data_ingestion_streaming]
    paths:
      - 'terraform/dev/**'
      - 'terraform/modules/data_ingestion/**'
      - 'terraform/modules/s3_buckets/**'
      - '.github/workflows/deploy_data_ingestion.yml'
      - 'terraform/assets/**'


permissions:
  contents: read

jobs:
  terraform:
    name: Apply S3 Buckets and Batch Ingestion
    runs-on: ubuntu-latest

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ap-southeast-2
      TF_VAR_snowflake_user: ${{ secrets.SNOWFLAKE_USER }}
      TF_VAR_snowflake_password: ${{ secrets.SNOWFLAKE_PASSWORD }}
      TF_VAR_snowflake_account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      TF_VAR_snowflake_role: ${{ secrets.SNOWFLAKE_ROLE }}
      TF_VAR_snowflake_warehouse: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

      TF_VAR_aws_region: ap-southeast-2
      TF_VAR_raw_bucket: insightflow-raw-bucket
      TF_VAR_clean_bucket: insightflow-clean-bucket

      ### 以下变量已经在main.tf中显式定义
      # TF_VAR_lambda_function_name: batch_ingestion
      # TF_VAR_lambda_handler: lambda_function.lambda_handler
      # TF_VAR_lambda_runtime: python3.13
      # TF_VAR_lambda_zip_path: ./assets/batch_ingestion_lambda.zip
      # TF_VAR_lambda_timeout: 900
      # TF_VAR_lambda_memory_size: 1024

      # TF_VAR_eventbridge_rule_name: batch_ingestion_trigger
      # TF_VAR_eventbridge_rule_description: "Trigger batch ingestion Lambda on 30th of every month at 00:00 Sydney time"
      # TF_VAR_eventbridge_schedule_expression: cron(0 14 30 * ? *)

      # TF_VAR_publisher_function_name: streaming_data_publisher
      # TF_VAR_publisher_zip_path: ./assets/streaming_data_publisher.zip
      # TF_VAR_publisher_handler: streaming_data_publisher.lambda_handler
      # TF_VAR_publisher_runtime: python3.13
      # TF_VAR_publisher_memory_size: 128
      # TF_VAR_publisher_timeout: 60

      # TF_VAR_transformer_function_name: streaming_data_transformer
      # TF_VAR_transformer_zip_path: ./assets/streaming_data_transformer.zip
      # TF_VAR_transformer_handler: streaming_data_transformer.lambda_handler
      # TF_VAR_transformer_runtime: python3.13
      # TF_VAR_transformer_memory_size: 128
      # TF_VAR_transformer_timeout: 60

      # TF_VAR_firehose_name: insightflow-dummy-firehose



    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.7.5

      - name: Terraform Init
        run: terraform init
        working-directory: terraform/dev
      
      - name: Terraform Format Check
        run: terraform fmt -check
        working-directory: terraform/dev

      - name: Terraform Plan (S3 Buckets Only)
        run: terraform plan -input=false -target=module.s3_buckets
        working-directory: terraform/dev

      - name: Terraform Apply (S3 Buckets Only)
        run: terraform apply -auto-approve -target=module.s3_buckets
        working-directory: terraform/dev

      - name: Terraform Plan (Batch Ingestion Only)
        run: terraform plan -input=false -target=module.batch_ingestion
        working-directory: terraform/dev

      - name: Terraform Apply (Batch Ingestion Only)
        run: terraform apply -auto-approve -target=module.batch_ingestion
        working-directory: terraform/dev

      - name: Terraform Plan (Streaming Ingestion Only)
        run: terraform plan -input=false -target=module.streaming_ingestion
        working-directory: terraform/dev

      - name: Terraform Apply (Streaming Ingestion Only)
        run: terraform apply -auto-approve -target=module.streaming_ingestion
        working-directory: terraform/dev